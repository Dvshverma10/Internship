{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17bea5d4",
   "metadata": {},
   "source": [
    "**Assignement : Web Scrapping - 4**\n",
    "\n",
    "**Devesh Verma**\n",
    "\n",
    "**23/June/2023 to 1/July/2023**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6dcdc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store required URLS\n",
    "\n",
    "url1 = 'https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos'\n",
    "\n",
    "url2 = 'http://www.bcci.tv/'\n",
    "\n",
    "url3 = 'http://statisticstimes.com'\n",
    "\n",
    "url4 = 'https://github.com/'\n",
    "\n",
    "url5 = 'https://www.billboard.com/'\n",
    "\n",
    "url6= 'https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare'\n",
    "\n",
    "url7 ='https://www.imdb.com/list/ls095964455/'\n",
    "\n",
    "url8 = 'https://archive.ics.uci.edu/'\n",
    "\n",
    "url9 = 'https://www.naukri.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52ebca48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import common Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2d6c2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT EXCEPTIONS\n",
    "\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import ElementNotInteractableException\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.common.exceptions import SessionNotCreatedException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c9f40f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to ChromeDriver\n",
    "\n",
    "driver = webdriver.Chrome(r'Chromedriver.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ddc1bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Load URL & open webdriver\n",
    "\n",
    "def load_url(url):\n",
    "\n",
    "    driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff5c8b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get Elements text or url using Path name or class name\n",
    "\n",
    "def get_all_ele(is_XPATH, ele_name, is_URL):\n",
    "    \n",
    "    try:\n",
    "        delay = 3\n",
    "        if(is_XPATH):\n",
    "            \n",
    "            out_ele  =  driver.find_elements(By.XPATH, ele_name)\n",
    "        \n",
    "        else:\n",
    "            out_ele = driver.find_elements(By.CLASS_NAME, ele_name)\n",
    "            \n",
    "            \n",
    "    except:\n",
    "        \n",
    "        print('Exception Raised at get_all_ele')\n",
    "        \n",
    "        if(is_XPATH):\n",
    "            WebDriverWait(driver, delay).until(EC.presence_of_element_located((By.XPATH,ele_name)))\n",
    "            out_ele  =  driver.find_elements(By.XPATH, ele_name)\n",
    "        \n",
    "        else:\n",
    "            WebDriverWait(driver, delay).until(EC.presence_of_element_located((By.CLASS_NAME,ele_name)))\n",
    "            out_ele = driver.find_elements(By.CLASS_NAME, ele_name)\n",
    "            \n",
    "    out_lst = []\n",
    "        \n",
    "    if(is_URL):\n",
    "        \n",
    "        for i in out_ele: \n",
    "            out_lst.append(i.get_attribute('href'))\n",
    "      \n",
    "    else:\n",
    "        \n",
    "        for i in out_ele:\n",
    "            out_lst.append(i.text)\n",
    "            \n",
    "    return out_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fe7b098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to send Keys\n",
    "\n",
    "def send_val(key_val,ele_name, is_XPATH):\n",
    "    delay=3\n",
    "    try:\n",
    "        if(is_XPATH):\n",
    "            out_ele = driver.find_element(By.XPATH, ele_name)\n",
    "            out_ele.send_keys(key_val)\n",
    "            \n",
    "        else:\n",
    "            out_ele = driver.find_element(By.CLASS_NAME, ele_name)\n",
    "            out_ele.send_keys(key_val)\n",
    "            \n",
    "    except:\n",
    "        print('Exception raised at send_val')\n",
    "        \n",
    "        if(is_XPATH):\n",
    "            WebDriverWait(driver, delay).until(EC.presence_of_element_located((By.XPATH,ele_name)))\n",
    "            out_ele = driver.find_element(By.XPATH, ele_name)\n",
    "            out_ele.send_keys(key_val)\n",
    "            \n",
    "        else:\n",
    "            WebDriverWait(driver, delay).until(EC.presence_of_element_located((By.CLASS_NAME,ele_name)))\n",
    "            out_ele = driver.find_element(By.CLASS_NAME, ele_name)\n",
    "            out_ele.send_keys(key_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ef1c89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Submit or click \n",
    "\n",
    "def sub_val(ele_name, is_XPATH):\n",
    "    \n",
    "    delay = 3\n",
    "    \n",
    "    try:\n",
    "        if(is_XPATH):\n",
    "            out_ele = driver.find_element(By.XPATH, ele_name)\n",
    "            out_ele.click()\n",
    "            \n",
    "        else:\n",
    "            out_ele = driver.find_element(By.CLASS_NAME, ele_name)\n",
    "            out_ele.click()\n",
    "        \n",
    "    except:\n",
    "        print('Exception Raised at sub_val')\n",
    "        \n",
    "        try:\n",
    "            if(is_XPATH):\n",
    "                WebDriverWait(driver, delay).until(EC.presence_of_element_located((By.XPATH,ele_name)))\n",
    "                out_ele = driver.find_element(By.XPATH, ele_name)\n",
    "                out_ele.click()\n",
    "        except:\n",
    "            print('Tried again, not found')\n",
    "        \n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                WebDriverWait(driver, delay).until(EC.presence_of_element_located((By.XPATH,ele_name)))\n",
    "                out_ele = driver.find_element(By.CLASS_NAME, ele_name)\n",
    "                out_ele.click()\n",
    "            \n",
    "            except:\n",
    "                print('Tried again, not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b2832ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for hovering actions\n",
    "\n",
    "def hover_to_ele(hover):\n",
    "    # Create an instance of ActionChains\n",
    "    actions = ActionChains(driver)\n",
    "    \n",
    "    try:\n",
    "        # Find the element you want to hover over\n",
    "        to_hover = driver.find_element(By.XPATH, hover)\n",
    "        time.sleep(2)\n",
    "        \n",
    "    except:\n",
    "        to_hover = driver.find_element(By.XPATH, hover)\n",
    "        \n",
    "        # Move the mouse to the element\n",
    "    actions.move_to_element(to_hover).perform()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462d4adf",
   "metadata": {},
   "source": [
    "1. Scrape the details of most viewed videos on YouTube from Wikipedia.\n",
    "\n",
    "Url = https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\n",
    "\n",
    "You need to find following details:\n",
    "A) Rank\n",
    "B) Name\n",
    "C) Artist\n",
    "D) Upload date\n",
    "E) Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69e73da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load URL\n",
    "load_url(url1)\n",
    "\n",
    "top_yt_det = []\n",
    "yt= []\n",
    "x= []\n",
    "\n",
    "# Store inspected elements\n",
    "\n",
    "tab_var = '//div[@class=\"mw-body-content mw-content-ltr\"]'\n",
    "\n",
    "# Get elements using XPATH\n",
    "try:\n",
    "    top_yt_det.append((get_all_ele(1,tab_var , 0))[0])\n",
    "    \n",
    "except:\n",
    "    print('Exception raised even after wait, check for spell errors')\n",
    "\n",
    "# Use For loop and append required elements \n",
    "for y in top_yt_det:\n",
    "    yt.append(y.split('\\n'))\n",
    "\n",
    "# Use For loop and append required elements \n",
    "for y in yt[0]:\n",
    "    x.append(y.split(','))\n",
    "\n",
    "rank = []\n",
    "vid = []\n",
    "art = []\n",
    "views = []\n",
    "up_date = []\n",
    "art= []\n",
    "artist = []\n",
    "\n",
    "# Use For loop and append required elements \n",
    "for a in range(10,40,1):\n",
    "\n",
    "    c = x[a]\n",
    "    rank.append((c[0].split('.'))[0])\n",
    "    vid.append((c[0].split('\"'))[1]) \n",
    "    views.append((c[0].split(' '))[-3])\n",
    "    up_date.append((c[0].split(' '))[-2]+' '+ (c[0].split(' '))[-1]+(c[1].split('['))[0])\n",
    "    art.append(((c[0].split(']'))[1]).split(' ')[1:-3])\n",
    "    \n",
    "# Use For loop and append required elements    \n",
    "for a in art:\n",
    "    x =''\n",
    "    for i in range(0,len(a),1):\n",
    "        x = x+a[i]+' '\n",
    "    artist.append(x)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56817280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Video</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Views (in Billions)</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Baby Shark Dance</td>\n",
       "      <td>Pinkfong Baby Shark - Kids' Songs &amp; Stories</td>\n",
       "      <td>12.85</td>\n",
       "      <td>June 17 2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Despacito</td>\n",
       "      <td>Luis Fonsi</td>\n",
       "      <td>8.16</td>\n",
       "      <td>January 12 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Johny Johny Yes Papa</td>\n",
       "      <td>LooLoo Kids</td>\n",
       "      <td>6.70</td>\n",
       "      <td>October 8 2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Bath Song</td>\n",
       "      <td>Cocomelon – Nursery Rhymes</td>\n",
       "      <td>6.20</td>\n",
       "      <td>May 2 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Shape of You</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>6.00</td>\n",
       "      <td>January 30 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>See You Again</td>\n",
       "      <td>Wiz Khalifa</td>\n",
       "      <td>5.89</td>\n",
       "      <td>April 6 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Phonics Song with Two Words</td>\n",
       "      <td>ChuChu TV</td>\n",
       "      <td>5.30</td>\n",
       "      <td>March 6 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Wheels on the Bus</td>\n",
       "      <td>Cocomelon – Nursery Rhymes</td>\n",
       "      <td>5.24</td>\n",
       "      <td>May 24 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Uptown Funk</td>\n",
       "      <td>Mark Ronson</td>\n",
       "      <td>4.92</td>\n",
       "      <td>November 19 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Learning Colors – Colorful Eggs on a Farm</td>\n",
       "      <td>Miroshka TV</td>\n",
       "      <td>4.89</td>\n",
       "      <td>February 27 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>Gangnam Style</td>\n",
       "      <td>Psy</td>\n",
       "      <td>4.80</td>\n",
       "      <td>July 15 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>Masha and the Bear – Recipe for Disaster</td>\n",
       "      <td>Get Movies</td>\n",
       "      <td>4.55</td>\n",
       "      <td>January 31 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>Dame Tu Cosita</td>\n",
       "      <td>El Chombo</td>\n",
       "      <td>4.35</td>\n",
       "      <td>April 5 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>Axel F</td>\n",
       "      <td>Crazy Frog</td>\n",
       "      <td>3.91</td>\n",
       "      <td>June 16 2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>Sugar</td>\n",
       "      <td>Maroon 5</td>\n",
       "      <td>3.87</td>\n",
       "      <td>January 14 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>Roar</td>\n",
       "      <td>Katy Perry</td>\n",
       "      <td>3.80</td>\n",
       "      <td>September 5 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>Counting Stars</td>\n",
       "      <td>OneRepublic</td>\n",
       "      <td>3.79</td>\n",
       "      <td>May 31 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>Sorry</td>\n",
       "      <td>Justin Bieber</td>\n",
       "      <td>3.66</td>\n",
       "      <td>October 22 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>Baa Baa Black Sheep</td>\n",
       "      <td>Cocomelon – Nursery Rhymes</td>\n",
       "      <td>3.64</td>\n",
       "      <td>June 25 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>Thinking Out Loud</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>3.60</td>\n",
       "      <td>October 7 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>Waka Waka (This Time for Africa)</td>\n",
       "      <td>Shakira</td>\n",
       "      <td>3.59</td>\n",
       "      <td>June 4 2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>Dark Horse</td>\n",
       "      <td>Katy Perry</td>\n",
       "      <td>3.52</td>\n",
       "      <td>February 20 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>Lakdi Ki Kathi</td>\n",
       "      <td>Jingle Toons</td>\n",
       "      <td>3.48</td>\n",
       "      <td>June 14 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>Faded</td>\n",
       "      <td>Alan Walker</td>\n",
       "      <td>3.45</td>\n",
       "      <td>December 3 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>Perfect</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>3.45</td>\n",
       "      <td>November 9 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>Let Her Go</td>\n",
       "      <td>Passenger</td>\n",
       "      <td>3.44</td>\n",
       "      <td>July 25 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>Girls Like You</td>\n",
       "      <td>Maroon 5</td>\n",
       "      <td>3.42</td>\n",
       "      <td>May 31 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>Humpty the train on a fruits ride</td>\n",
       "      <td>Kiddiestv Hindi – Nursery Rhymes &amp; Kids Songs</td>\n",
       "      <td>3.41</td>\n",
       "      <td>January 26 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>Lean On</td>\n",
       "      <td>Major Lazer</td>\n",
       "      <td>3.38</td>\n",
       "      <td>March 22 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>Bailando</td>\n",
       "      <td>Enrique Iglesias</td>\n",
       "      <td>3.38</td>\n",
       "      <td>April 11 2014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank                                      Video  \\\n",
       "0     1                           Baby Shark Dance   \n",
       "1     2                                  Despacito   \n",
       "2     3                       Johny Johny Yes Papa   \n",
       "3     4                                  Bath Song   \n",
       "4     5                               Shape of You   \n",
       "5     6                              See You Again   \n",
       "6     7                Phonics Song with Two Words   \n",
       "7     8                          Wheels on the Bus   \n",
       "8     9                                Uptown Funk   \n",
       "9    10  Learning Colors – Colorful Eggs on a Farm   \n",
       "10   11                              Gangnam Style   \n",
       "11   12   Masha and the Bear – Recipe for Disaster   \n",
       "12   13                             Dame Tu Cosita   \n",
       "13   14                                     Axel F   \n",
       "14   15                                      Sugar   \n",
       "15   16                                       Roar   \n",
       "16   17                             Counting Stars   \n",
       "17   18                                      Sorry   \n",
       "18   19                        Baa Baa Black Sheep   \n",
       "19   20                          Thinking Out Loud   \n",
       "20   21           Waka Waka (This Time for Africa)   \n",
       "21   22                                 Dark Horse   \n",
       "22   23                             Lakdi Ki Kathi   \n",
       "23   24                                      Faded   \n",
       "24   25                                    Perfect   \n",
       "25   26                                 Let Her Go   \n",
       "26   27                             Girls Like You   \n",
       "27   28          Humpty the train on a fruits ride   \n",
       "28   29                                    Lean On   \n",
       "29   30                                   Bailando   \n",
       "\n",
       "                                            Artist Views (in Billions)  \\\n",
       "0     Pinkfong Baby Shark - Kids' Songs & Stories                12.85   \n",
       "1                                      Luis Fonsi                 8.16   \n",
       "2                                     LooLoo Kids                 6.70   \n",
       "3                      Cocomelon – Nursery Rhymes                 6.20   \n",
       "4                                      Ed Sheeran                 6.00   \n",
       "5                                     Wiz Khalifa                 5.89   \n",
       "6                                       ChuChu TV                 5.30   \n",
       "7                      Cocomelon – Nursery Rhymes                 5.24   \n",
       "8                                     Mark Ronson                 4.92   \n",
       "9                                     Miroshka TV                 4.89   \n",
       "10                                            Psy                 4.80   \n",
       "11                                     Get Movies                 4.55   \n",
       "12                                      El Chombo                 4.35   \n",
       "13                                     Crazy Frog                 3.91   \n",
       "14                                       Maroon 5                 3.87   \n",
       "15                                     Katy Perry                 3.80   \n",
       "16                                    OneRepublic                 3.79   \n",
       "17                                  Justin Bieber                 3.66   \n",
       "18                     Cocomelon – Nursery Rhymes                 3.64   \n",
       "19                                     Ed Sheeran                 3.60   \n",
       "20                                        Shakira                 3.59   \n",
       "21                                     Katy Perry                 3.52   \n",
       "22                                   Jingle Toons                 3.48   \n",
       "23                                    Alan Walker                 3.45   \n",
       "24                                     Ed Sheeran                 3.45   \n",
       "25                                      Passenger                 3.44   \n",
       "26                                       Maroon 5                 3.42   \n",
       "27  Kiddiestv Hindi – Nursery Rhymes & Kids Songs                 3.41   \n",
       "28                                    Major Lazer                 3.38   \n",
       "29                               Enrique Iglesias                 3.38   \n",
       "\n",
       "                Date  \n",
       "0      June 17 2016   \n",
       "1   January 12 2017   \n",
       "2     October 8 2016  \n",
       "3         May 2 2018  \n",
       "4   January 30 2017   \n",
       "5      April 6 2015   \n",
       "6       March 6 2014  \n",
       "7        May 24 2018  \n",
       "8   November 19 2014  \n",
       "9   February 27 2018  \n",
       "10     July 15 2012   \n",
       "11   January 31 2012  \n",
       "12      April 5 2018  \n",
       "13      June 16 2009  \n",
       "14   January 14 2015  \n",
       "15  September 5 2013  \n",
       "16       May 31 2013  \n",
       "17   October 22 2015  \n",
       "18      June 25 2018  \n",
       "19    October 7 2014  \n",
       "20       June 4 2010  \n",
       "21  February 20 2014  \n",
       "22      June 14 2018  \n",
       "23   December 3 2015  \n",
       "24   November 9 2017  \n",
       "25      July 25 2012  \n",
       "26       May 31 2018  \n",
       "27   January 26 2018  \n",
       "28     March 22 2015  \n",
       "29     April 11 2014  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store in dictionary & Create Dataframe\n",
    "\n",
    "dict_wiki = {'Rank': rank, 'Video': vid, 'Artist': artist, 'Views (in Billions)': views, 'Date': up_date }\n",
    "\n",
    "wiki_DF = pd.DataFrame(dict_wiki)\n",
    "\n",
    "wiki_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ad9654",
   "metadata": {},
   "source": [
    "2. Scrape the details team India’s international fixtures from bcci.tv\n",
    "\n",
    "Url = https://www.bcci.tv/\n",
    "\n",
    "You need to find following details:\n",
    "\n",
    "A) Match title (I.e. 1stODI)\n",
    "\n",
    "B) Series\n",
    "\n",
    "C) Place\n",
    "\n",
    "D) Date\n",
    "\n",
    "E) Time\n",
    "\n",
    "Note: - From bcci.tv home page you have reach to the international fixture page through code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55f89e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception Raised at sub_val\n",
      "Tried again, not found\n"
     ]
    }
   ],
   "source": [
    "# Go to URL & Navigate using click functions\n",
    "\n",
    "load_url(url2)\n",
    "\n",
    "sub_val('/html/body/nav/div[1]/div[2]/ul[1]/li[2]/a',1)\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "sub_val('/html/body/div[2]/div[2]/div/div/div/div[2]/div[3]/div[2]/div/button',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3167592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Match</th>\n",
       "      <th>Series</th>\n",
       "      <th>Place</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1st T20I</td>\n",
       "      <td>INDIA WOMEN TOUR OF BANGLADESH 2023</td>\n",
       "      <td>Shere Bangla National Stadium Mirpur</td>\n",
       "      <td>9 JUL 2023</td>\n",
       "      <td>1:30 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2nd T20I</td>\n",
       "      <td>INDIA WOMEN TOUR OF BANGLADESH 2023</td>\n",
       "      <td>Shere Bangla National Stadium Mirpur</td>\n",
       "      <td>11 JUL 2023</td>\n",
       "      <td>1:30 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1st Test</td>\n",
       "      <td>INDIA TOUR OF WEST INDIES 2023</td>\n",
       "      <td>Windsor Park Dominica</td>\n",
       "      <td>12 JUL 2023</td>\n",
       "      <td>7:30 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3rd T20I</td>\n",
       "      <td>INDIA WOMEN TOUR OF BANGLADESH 2023</td>\n",
       "      <td>Shere Bangla National Stadium Mirpur</td>\n",
       "      <td>13 JUL 2023</td>\n",
       "      <td>1:30 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1st ODI</td>\n",
       "      <td>INDIA WOMEN TOUR OF BANGLADESH 2023</td>\n",
       "      <td>Shere Bangla National Stadium Mirpur</td>\n",
       "      <td>16 JUL 2023</td>\n",
       "      <td>9:00 AM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2nd ODI</td>\n",
       "      <td>INDIA WOMEN TOUR OF BANGLADESH 2023</td>\n",
       "      <td>Shere Bangla National Stadium Mirpur</td>\n",
       "      <td>19 JUL 2023</td>\n",
       "      <td>9:00 AM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2nd Test</td>\n",
       "      <td>INDIA TOUR OF WEST INDIES 2023</td>\n",
       "      <td>Queen's Park Oval Trinidad</td>\n",
       "      <td>20 JUL 2023</td>\n",
       "      <td>7:30 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3rd ODI</td>\n",
       "      <td>INDIA WOMEN TOUR OF BANGLADESH 2023</td>\n",
       "      <td>Shere Bangla National Stadium Mirpur</td>\n",
       "      <td>22 JUL 2023</td>\n",
       "      <td>9:00 AM IST</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Match                               Series  \\\n",
       "0  1st T20I   INDIA WOMEN TOUR OF BANGLADESH 2023   \n",
       "1  2nd T20I   INDIA WOMEN TOUR OF BANGLADESH 2023   \n",
       "2  1st Test        INDIA TOUR OF WEST INDIES 2023   \n",
       "3  3rd T20I   INDIA WOMEN TOUR OF BANGLADESH 2023   \n",
       "4   1st ODI   INDIA WOMEN TOUR OF BANGLADESH 2023   \n",
       "5   2nd ODI   INDIA WOMEN TOUR OF BANGLADESH 2023   \n",
       "6  2nd Test        INDIA TOUR OF WEST INDIES 2023   \n",
       "7   3rd ODI   INDIA WOMEN TOUR OF BANGLADESH 2023   \n",
       "\n",
       "                                   Place         Date         Time  \n",
       "0   Shere Bangla National Stadium Mirpur   9 JUL 2023  1:30 PM IST  \n",
       "1   Shere Bangla National Stadium Mirpur  11 JUL 2023  1:30 PM IST  \n",
       "2                  Windsor Park Dominica  12 JUL 2023  7:30 PM IST  \n",
       "3   Shere Bangla National Stadium Mirpur  13 JUL 2023  1:30 PM IST  \n",
       "4   Shere Bangla National Stadium Mirpur  16 JUL 2023  9:00 AM IST  \n",
       "5   Shere Bangla National Stadium Mirpur  19 JUL 2023  9:00 AM IST  \n",
       "6             Queen's Park Oval Trinidad  20 JUL 2023  7:30 PM IST  \n",
       "7   Shere Bangla National Stadium Mirpur  22 JUL 2023  9:00 AM IST  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting the data and storing in the list\n",
    "\n",
    "m_data = get_all_ele(0, 'match-card-top', 0)\n",
    "m_data_A = get_all_ele(0, 'match-card-bottom', 0)\n",
    "\n",
    "m_series = []\n",
    "m_date = []\n",
    "m_time = []\n",
    "m_title = []\n",
    "m_place = []\n",
    "\n",
    "# Filter and append required data \n",
    "for m in m_data:\n",
    "    m_series.append(m.split('\\n')[0])\n",
    "    m_date.append(m.split('\\n')[1])\n",
    "    m_time.append(m.split('\\n')[2])\n",
    "    \n",
    "for m in m_data_A:\n",
    "    m_title.append((m.split(',')[0]).split('-')[0])\n",
    "    m_place.append((m.split(',')[0]).split('-')[1] + (m.split(',')[1]).split('\\n')[0])\n",
    "\n",
    "# Store in dictionary & create DataFrame\n",
    "\n",
    "dict_bcci = {'Match': m_title, 'Series': m_series, 'Place': m_place, 'Date': m_date, 'Time': m_time}\n",
    "\n",
    "bcci_DF = pd.DataFrame(dict_bcci)\n",
    "\n",
    "bcci_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86e142a",
   "metadata": {},
   "source": [
    "3. Scrape the details of State-wise GDP ofIndia fromstatisticstime.com\n",
    "\n",
    "Url = http://statisticstimes.com/\n",
    "\n",
    "You have to find following details:\n",
    "\n",
    "A) Rank\n",
    "\n",
    "B) State\n",
    "\n",
    "C) GSDP(18-19)- at current prices\n",
    "\n",
    "D) GSDP(19-20)- at current prices\n",
    "\n",
    "E) Share(18-19)\n",
    "\n",
    "F) GDP($ billion)\n",
    "\n",
    "Note: - From statisticstimes home page you have to reach to economy page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2ca155c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go to URL & Navigate using hover & click functions\n",
    "\n",
    "load_url(url3)\n",
    "\n",
    "time.sleep(1)\n",
    "hover = '/html/body/div[2]/div[1]/div[2]/div[2]/button'\n",
    "hover_to_ele(hover)\n",
    "\n",
    "time.sleep(1)\n",
    "sub_val('/html/body/div[2]/div[1]/div[2]/div[2]/div/a[3]',1)\n",
    "time.sleep(3)\n",
    "sub_val('/html/body/div[2]/div[2]/div[2]/ul/li[1]/a',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf430a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing & Extracting the elements\n",
    "\n",
    "gdp_lst = []\n",
    "gdp_lst.append(get_all_ele(1, '//div[@class=\"dataTables_wrapper\"]',0))\n",
    "\n",
    "x = []\n",
    "\n",
    "for rows in gdp_lst[0]: \n",
    "    x.append(rows.split('\\n'))\n",
    "\n",
    "# Removing Heading Data\n",
    "gdp_all = x[0][3:-1]\n",
    "\n",
    "rank = []\n",
    "state = []\n",
    "gs18_19_cp = []\n",
    "gs19_20_cp = []\n",
    "share = []\n",
    "gp_bn= []\n",
    "\n",
    "# Run for loop and extract data in list\n",
    "for st in gdp_all:\n",
    "    \n",
    "    y = st.split(' ')\n",
    "    z = ''\n",
    "    \n",
    "    rank.append(y[0])\n",
    "    gs19_20_cp.append(y[-6])\n",
    "    gs18_19_cp.append(y[-5])\n",
    "    share.append(y[-4])\n",
    "    gp_bn.append(y[-3])\n",
    "    \n",
    "    for k in range(0, len(y)-7):\n",
    "        z = z + y[k+1]\n",
    "        \n",
    "    state.append(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b5cdbb63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>State</th>\n",
       "      <th>GSDP(18-19 at CP)</th>\n",
       "      <th>GSDP(19-20 at CP)</th>\n",
       "      <th>Share</th>\n",
       "      <th>GDP($ Bn)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>2,632,792</td>\n",
       "      <td>-</td>\n",
       "      <td>13.94%</td>\n",
       "      <td>399.921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>TamilNadu</td>\n",
       "      <td>1,630,208</td>\n",
       "      <td>1,845,853</td>\n",
       "      <td>8.63%</td>\n",
       "      <td>247.629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>UttarPradesh</td>\n",
       "      <td>1,584,764</td>\n",
       "      <td>1,687,818</td>\n",
       "      <td>8.39%</td>\n",
       "      <td>240.726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Gujarat</td>\n",
       "      <td>1,502,899</td>\n",
       "      <td>-</td>\n",
       "      <td>7.96%</td>\n",
       "      <td>228.290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Karnataka</td>\n",
       "      <td>1,493,127</td>\n",
       "      <td>1,631,977</td>\n",
       "      <td>7.91%</td>\n",
       "      <td>226.806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>WestBengal</td>\n",
       "      <td>1,089,898</td>\n",
       "      <td>1,253,832</td>\n",
       "      <td>5.77%</td>\n",
       "      <td>165.556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Rajasthan</td>\n",
       "      <td>942,586</td>\n",
       "      <td>1,020,989</td>\n",
       "      <td>4.99%</td>\n",
       "      <td>143.179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>AndhraPradesh</td>\n",
       "      <td>862,957</td>\n",
       "      <td>972,782</td>\n",
       "      <td>4.57%</td>\n",
       "      <td>131.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Telangana</td>\n",
       "      <td>861,031</td>\n",
       "      <td>969,604</td>\n",
       "      <td>4.56%</td>\n",
       "      <td>130.791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>MadhyaPradesh</td>\n",
       "      <td>809,592</td>\n",
       "      <td>906,672</td>\n",
       "      <td>4.29%</td>\n",
       "      <td>122.977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>Kerala</td>\n",
       "      <td>781,653</td>\n",
       "      <td>-</td>\n",
       "      <td>4.14%</td>\n",
       "      <td>118.733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>774,870</td>\n",
       "      <td>856,112</td>\n",
       "      <td>4.10%</td>\n",
       "      <td>117.703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>Haryana</td>\n",
       "      <td>734,163</td>\n",
       "      <td>831,610</td>\n",
       "      <td>3.89%</td>\n",
       "      <td>111.519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>Bihar</td>\n",
       "      <td>530,363</td>\n",
       "      <td>611,804</td>\n",
       "      <td>2.81%</td>\n",
       "      <td>80.562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>Punjab</td>\n",
       "      <td>526,376</td>\n",
       "      <td>574,760</td>\n",
       "      <td>2.79%</td>\n",
       "      <td>79.957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>Odisha</td>\n",
       "      <td>487,805</td>\n",
       "      <td>521,275</td>\n",
       "      <td>2.58%</td>\n",
       "      <td>74.098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>Assam</td>\n",
       "      <td>315,881</td>\n",
       "      <td>-</td>\n",
       "      <td>1.67%</td>\n",
       "      <td>47.982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>Chhattisgarh</td>\n",
       "      <td>304,063</td>\n",
       "      <td>329,180</td>\n",
       "      <td>1.61%</td>\n",
       "      <td>46.187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>Jharkhand</td>\n",
       "      <td>297,204</td>\n",
       "      <td>328,598</td>\n",
       "      <td>1.57%</td>\n",
       "      <td>45.145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>Uttarakhand</td>\n",
       "      <td>245,895</td>\n",
       "      <td>-</td>\n",
       "      <td>1.30%</td>\n",
       "      <td>37.351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>Jammu&amp;Kashmir</td>\n",
       "      <td>155,956</td>\n",
       "      <td>-</td>\n",
       "      <td>0.83%</td>\n",
       "      <td>23.690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>HimachalPradesh</td>\n",
       "      <td>153,845</td>\n",
       "      <td>165,472</td>\n",
       "      <td>0.81%</td>\n",
       "      <td>23.369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>Goa</td>\n",
       "      <td>73,170</td>\n",
       "      <td>80,449</td>\n",
       "      <td>0.39%</td>\n",
       "      <td>11.115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>Tripura</td>\n",
       "      <td>49,845</td>\n",
       "      <td>55,984</td>\n",
       "      <td>0.26%</td>\n",
       "      <td>7.571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>Chandigarh</td>\n",
       "      <td>42,114</td>\n",
       "      <td>-</td>\n",
       "      <td>0.22%</td>\n",
       "      <td>6.397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>Puducherry</td>\n",
       "      <td>34,433</td>\n",
       "      <td>38,253</td>\n",
       "      <td>0.18%</td>\n",
       "      <td>5.230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>Meghalaya</td>\n",
       "      <td>33,481</td>\n",
       "      <td>36,572</td>\n",
       "      <td>0.18%</td>\n",
       "      <td>5.086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>Sikkim</td>\n",
       "      <td>28,723</td>\n",
       "      <td>32,496</td>\n",
       "      <td>0.15%</td>\n",
       "      <td>4.363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>Manipur</td>\n",
       "      <td>27,870</td>\n",
       "      <td>31,790</td>\n",
       "      <td>0.15%</td>\n",
       "      <td>4.233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>Nagaland</td>\n",
       "      <td>27,283</td>\n",
       "      <td>-</td>\n",
       "      <td>0.14%</td>\n",
       "      <td>4.144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>ArunachalPradesh</td>\n",
       "      <td>24,603</td>\n",
       "      <td>-</td>\n",
       "      <td>0.13%</td>\n",
       "      <td>3.737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>Mizoram</td>\n",
       "      <td>22,287</td>\n",
       "      <td>26,503</td>\n",
       "      <td>0.12%</td>\n",
       "      <td>3.385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>Andaman&amp;NicobarIslands</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank                   State GSDP(18-19 at CP) GSDP(19-20 at CP)   Share  \\\n",
       "0     1             Maharashtra         2,632,792                 -  13.94%   \n",
       "1     2               TamilNadu         1,630,208         1,845,853   8.63%   \n",
       "2     3            UttarPradesh         1,584,764         1,687,818   8.39%   \n",
       "3     4                 Gujarat         1,502,899                 -   7.96%   \n",
       "4     5               Karnataka         1,493,127         1,631,977   7.91%   \n",
       "5     6              WestBengal         1,089,898         1,253,832   5.77%   \n",
       "6     7               Rajasthan           942,586         1,020,989   4.99%   \n",
       "7     8           AndhraPradesh           862,957           972,782   4.57%   \n",
       "8     9               Telangana           861,031           969,604   4.56%   \n",
       "9    10           MadhyaPradesh           809,592           906,672   4.29%   \n",
       "10   11                  Kerala           781,653                 -   4.14%   \n",
       "11   12                   Delhi           774,870           856,112   4.10%   \n",
       "12   13                 Haryana           734,163           831,610   3.89%   \n",
       "13   14                   Bihar           530,363           611,804   2.81%   \n",
       "14   15                  Punjab           526,376           574,760   2.79%   \n",
       "15   16                  Odisha           487,805           521,275   2.58%   \n",
       "16   17                   Assam           315,881                 -   1.67%   \n",
       "17   18            Chhattisgarh           304,063           329,180   1.61%   \n",
       "18   19               Jharkhand           297,204           328,598   1.57%   \n",
       "19   20             Uttarakhand           245,895                 -   1.30%   \n",
       "20   21           Jammu&Kashmir           155,956                 -   0.83%   \n",
       "21   22         HimachalPradesh           153,845           165,472   0.81%   \n",
       "22   23                     Goa            73,170            80,449   0.39%   \n",
       "23   24                 Tripura            49,845            55,984   0.26%   \n",
       "24   25              Chandigarh            42,114                 -   0.22%   \n",
       "25   26              Puducherry            34,433            38,253   0.18%   \n",
       "26   27               Meghalaya            33,481            36,572   0.18%   \n",
       "27   28                  Sikkim            28,723            32,496   0.15%   \n",
       "28   29                 Manipur            27,870            31,790   0.15%   \n",
       "29   30                Nagaland            27,283                 -   0.14%   \n",
       "30   31        ArunachalPradesh            24,603                 -   0.13%   \n",
       "31   32                 Mizoram            22,287            26,503   0.12%   \n",
       "32   33  Andaman&NicobarIslands                 -                 -       -   \n",
       "\n",
       "   GDP($ Bn)  \n",
       "0    399.921  \n",
       "1    247.629  \n",
       "2    240.726  \n",
       "3    228.290  \n",
       "4    226.806  \n",
       "5    165.556  \n",
       "6    143.179  \n",
       "7    131.083  \n",
       "8    130.791  \n",
       "9    122.977  \n",
       "10   118.733  \n",
       "11   117.703  \n",
       "12   111.519  \n",
       "13    80.562  \n",
       "14    79.957  \n",
       "15    74.098  \n",
       "16    47.982  \n",
       "17    46.187  \n",
       "18    45.145  \n",
       "19    37.351  \n",
       "20    23.690  \n",
       "21    23.369  \n",
       "22    11.115  \n",
       "23     7.571  \n",
       "24     6.397  \n",
       "25     5.230  \n",
       "26     5.086  \n",
       "27     4.363  \n",
       "28     4.233  \n",
       "29     4.144  \n",
       "30     3.737  \n",
       "31     3.385  \n",
       "32         -  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating Dictionary & Storing in DataFrame\n",
    "\n",
    "dict_gdp = {'Rank': rank, 'State': state, 'GSDP(18-19 at CP)': gs18_19_cp,'GSDP(19-20 at CP)': gs19_20_cp, 'Share': share, 'GDP($ Bn)':gp_bn}\n",
    "\n",
    "gdp_DF = pd.DataFrame(dict_gdp)\n",
    "\n",
    "gdp_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32f92e1",
   "metadata": {},
   "source": [
    "4. Scrape the details of trending repositories on Github.com\n",
    "\n",
    "Url = https://github.com/\n",
    "\n",
    "You have to find the following details:\n",
    "\n",
    "A) Repository title\n",
    "\n",
    "B) Repository description\n",
    "\n",
    "C) Contributors count\n",
    "\n",
    "D) Language used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e5149db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go to URL & Navigate using hover & click functions\n",
    "\n",
    "load_url(url4)\n",
    "\n",
    "time.sleep(1)\n",
    "hover = '/html/body/div[1]/div[1]/header/div/div[2]/div/nav/ul/li[3]/button'\n",
    "hover_to_ele(hover)\n",
    "\n",
    "time.sleep(2)\n",
    "sub_val('/html/body/div[1]/div[1]/header/div/div[2]/div/nav/ul/li[3]/div/div[3]/ul/li[2]/a',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "474d674e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Empty list to store data\n",
    "\n",
    "grep_ttl = []\n",
    "grep_cct = []\n",
    "grepA_des = []\n",
    "grepA_lng = []\n",
    "\n",
    "# Extract and filter out elements\n",
    "\n",
    "grep =  get_all_ele(1, '//span[@class=\"text-normal\"]',0)\n",
    "\n",
    "for title in grep:\n",
    "    grep_ttl.append(title.replace(' /', ''))\n",
    "\n",
    "# Extract and filter out elements\n",
    "grep2 =  get_all_ele(1, '//span[@class=\"d-inline-block ml-0 mr-3\"]',0)\n",
    "grep3 =  get_all_ele(1, '//a[@class=\"Link--muted d-inline-block mr-3\"]',0)\n",
    "\n",
    "i = 0\n",
    "\n",
    "for cct in grep3: \n",
    "    i = i+1\n",
    "    \n",
    "    if(i%2 ==0):\n",
    "        grep_cct.append(cct)\n",
    "        \n",
    "grepA= get_all_ele(1,'//article[@class=\"Box-row\"]',0)\n",
    "\n",
    "for des in grepA:\n",
    "    \n",
    "    z = des.split('\\n')\n",
    "    \n",
    "    if(len(z) == 6):\n",
    "        grepA_des.append(z[3])\n",
    "        grepA_lng.append(z[4].split(' ')[0])\n",
    "        \n",
    "    elif(len(z) == 4):\n",
    "        grepA_des.append('--')\n",
    "        grepA_lng.append('--')\n",
    "    \n",
    "    else:\n",
    "        grepA_des.append(z[2])\n",
    "        grepA_lng.append(z[3].split(' ')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e3a8a641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Repository</th>\n",
       "      <th>Description</th>\n",
       "      <th>Count</th>\n",
       "      <th>Language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PowerShell</td>\n",
       "      <td>PowerShell for every system!</td>\n",
       "      <td>7,162</td>\n",
       "      <td>C#</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WeMakeDevs</td>\n",
       "      <td>--</td>\n",
       "      <td>365</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>toeverything</td>\n",
       "      <td>There can be more than Notion and Miro. AFFiNE...</td>\n",
       "      <td>998</td>\n",
       "      <td>TypeScript</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>facebook</td>\n",
       "      <td>An open-source C++ library developed and used ...</td>\n",
       "      <td>5,345</td>\n",
       "      <td>C++</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ChaoningZhang</td>\n",
       "      <td>This is the official code for Faster Segment A...</td>\n",
       "      <td>701</td>\n",
       "      <td>Jupyter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>imgly</td>\n",
       "      <td>Remove backgrounds from images directly in the...</td>\n",
       "      <td>75</td>\n",
       "      <td>TypeScript</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>practical-tutorials</td>\n",
       "      <td>Curated list of project-based tutorials</td>\n",
       "      <td>15,179</td>\n",
       "      <td>108,551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>wgwang</td>\n",
       "      <td>中国大模型</td>\n",
       "      <td>111</td>\n",
       "      <td>736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>OpenDriveLab</td>\n",
       "      <td>All you need for End-to-end Autonomous Driving</td>\n",
       "      <td>22</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ripienaar</td>\n",
       "      <td>A list of SaaS, PaaS and IaaS offerings that h...</td>\n",
       "      <td>7,759</td>\n",
       "      <td>HTML</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>homanp</td>\n",
       "      <td>🥷 Superagent - Build, deploy, and manage LLM-p...</td>\n",
       "      <td>187</td>\n",
       "      <td>JavaScript</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>hiyouga</td>\n",
       "      <td>Fine-tuning ChatGLM-6B with PEFT | 基于 PEFT 的高效...</td>\n",
       "      <td>236</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>EbookFoundation</td>\n",
       "      <td>📚 Freely available programming books</td>\n",
       "      <td>55,972</td>\n",
       "      <td>285,232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>teslamotors</td>\n",
       "      <td>--</td>\n",
       "      <td>18</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>labmlai</td>\n",
       "      <td>🧑‍🏫 59 Implementations/tutorials of deep learn...</td>\n",
       "      <td>2,834</td>\n",
       "      <td>Jupyter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>huggingface</td>\n",
       "      <td>🤗 Transformers: State-of-the-art Machine Learn...</td>\n",
       "      <td>21,204</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>StanGirard</td>\n",
       "      <td>🧠 Dump all your files into your private Genera...</td>\n",
       "      <td>1,144</td>\n",
       "      <td>TypeScript</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>dotnet-architecture</td>\n",
       "      <td>Cross-platform .NET sample microservices and c...</td>\n",
       "      <td>10,147</td>\n",
       "      <td>C#</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>rust-lang</td>\n",
       "      <td>Create book from markdown files. Like Gitbook ...</td>\n",
       "      <td>1,396</td>\n",
       "      <td>Rust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LiLittleCat</td>\n",
       "      <td>🆓免费的 ChatGPT 镜像网站列表，持续更新。List of free ChatGPT ...</td>\n",
       "      <td>539</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>geerlingguy</td>\n",
       "      <td>Ansible for DevOps examples.</td>\n",
       "      <td>2,997</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>phodal</td>\n",
       "      <td>《构筑大语言模型应用：应用开发与架构设计》一本关于 LLM 在真实世界应用的开源电子书，介绍...</td>\n",
       "      <td>42</td>\n",
       "      <td>Rust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>PromtEngineer</td>\n",
       "      <td>Chat with your documents on your local device ...</td>\n",
       "      <td>795</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Visualize-ML</td>\n",
       "      <td>Book_4_《矩阵力量》 | 鸢尾花书：从加减乘除到机器学习；上架！</td>\n",
       "      <td>524</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>jetpack-io</td>\n",
       "      <td>Type-safe, K-sortable, globally unique identif...</td>\n",
       "      <td>18</td>\n",
       "      <td>Go</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Repository                                        Description  \\\n",
       "0            PowerShell                       PowerShell for every system!   \n",
       "1            WeMakeDevs                                                 --   \n",
       "2          toeverything  There can be more than Notion and Miro. AFFiNE...   \n",
       "3              facebook  An open-source C++ library developed and used ...   \n",
       "4         ChaoningZhang  This is the official code for Faster Segment A...   \n",
       "5                 imgly  Remove backgrounds from images directly in the...   \n",
       "6   practical-tutorials            Curated list of project-based tutorials   \n",
       "7                wgwang                                              中国大模型   \n",
       "8          OpenDriveLab     All you need for End-to-end Autonomous Driving   \n",
       "9             ripienaar  A list of SaaS, PaaS and IaaS offerings that h...   \n",
       "10               homanp  🥷 Superagent - Build, deploy, and manage LLM-p...   \n",
       "11              hiyouga  Fine-tuning ChatGLM-6B with PEFT | 基于 PEFT 的高效...   \n",
       "12      EbookFoundation               📚 Freely available programming books   \n",
       "13          teslamotors                                                 --   \n",
       "14              labmlai  🧑‍🏫 59 Implementations/tutorials of deep learn...   \n",
       "15          huggingface  🤗 Transformers: State-of-the-art Machine Learn...   \n",
       "16           StanGirard  🧠 Dump all your files into your private Genera...   \n",
       "17  dotnet-architecture  Cross-platform .NET sample microservices and c...   \n",
       "18            rust-lang  Create book from markdown files. Like Gitbook ...   \n",
       "19          LiLittleCat  🆓免费的 ChatGPT 镜像网站列表，持续更新。List of free ChatGPT ...   \n",
       "20          geerlingguy                       Ansible for DevOps examples.   \n",
       "21               phodal  《构筑大语言模型应用：应用开发与架构设计》一本关于 LLM 在真实世界应用的开源电子书，介绍...   \n",
       "22        PromtEngineer  Chat with your documents on your local device ...   \n",
       "23         Visualize-ML                Book_4_《矩阵力量》 | 鸢尾花书：从加减乘除到机器学习；上架！   \n",
       "24           jetpack-io  Type-safe, K-sortable, globally unique identif...   \n",
       "\n",
       "     Count    Language  \n",
       "0    7,162          C#  \n",
       "1      365          --  \n",
       "2      998  TypeScript  \n",
       "3    5,345         C++  \n",
       "4      701     Jupyter  \n",
       "5       75  TypeScript  \n",
       "6   15,179     108,551  \n",
       "7      111         736  \n",
       "8       22         336  \n",
       "9    7,759        HTML  \n",
       "10     187  JavaScript  \n",
       "11     236      Python  \n",
       "12  55,972     285,232  \n",
       "13      18          --  \n",
       "14   2,834     Jupyter  \n",
       "15  21,204      Python  \n",
       "16   1,144  TypeScript  \n",
       "17  10,147          C#  \n",
       "18   1,396        Rust  \n",
       "19     539      Python  \n",
       "20   2,997      Python  \n",
       "21      42        Rust  \n",
       "22     795      Python  \n",
       "23     524      Python  \n",
       "24      18          Go  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dictionary & store in DataFrame\n",
    "dict_GHUB = {'Repository': grep_ttl, 'Description': grepA_des, 'Count': grep_cct, 'Language': grepA_lng}\n",
    "\n",
    "ghub_DF = pd.DataFrame(dict_GHUB)\n",
    "\n",
    "ghub_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ac8228",
   "metadata": {},
   "source": [
    "5. Scrape the details of top 100 songs on billiboard.com\n",
    "\n",
    "Url = https:/www.billboard.com/\n",
    "\n",
    "You have to find the following details:\n",
    "\n",
    "A) Song name\n",
    "\n",
    "B) Artist name\n",
    "\n",
    "C) Last week rank\n",
    "\n",
    "D) Peak rank\n",
    "\n",
    "E) Weeks on board\n",
    "\n",
    "Note: - From the home page you have to click on the charts option then hot 100-page link through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "112b6b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception Raised at sub_val\n",
      "Tried again, not found\n"
     ]
    }
   ],
   "source": [
    "# Go to URL & Navigate using hover & click functions\n",
    "\n",
    "load_url(url5)\n",
    "time.sleep(1)\n",
    "sub_val('/html/body/div[3]/header/div/div[2]/div/div/div[2]/div[2]/div/div/nav/ul/li[1]/a',1)\n",
    "\n",
    "time.sleep(3)\n",
    "sub_val('/html/body/div[3]/main/div[2]/div[3]/div/div/div[2]/div/div[1]/a/div/div[1]/div/img',1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bafb5d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Song</th>\n",
       "      <th>Artist</th>\n",
       "      <th>LastWeekRank</th>\n",
       "      <th>PeakRank</th>\n",
       "      <th>WeeksOnboard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Last Night</td>\n",
       "      <td>Morgan Wallen</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Fast Car</td>\n",
       "      <td>Luke Combs</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Calm Down</td>\n",
       "      <td>Rema &amp; Selena Gomez</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Flowers</td>\n",
       "      <td>Miley Cyrus</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>All My Life</td>\n",
       "      <td>Lil Durk Featuring J. Cole</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>RE- ENTRY</td>\n",
       "      <td>Angel, Pt. 1</td>\n",
       "      <td>Kodak Black, NLE Choppa, Jimin, JVKE &amp; Muni Long</td>\n",
       "      <td>-</td>\n",
       "      <td>65</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>NEW</td>\n",
       "      <td>Girl In Mine</td>\n",
       "      <td>Parmalee</td>\n",
       "      <td>-</td>\n",
       "      <td>97</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>Moonlight</td>\n",
       "      <td>Kali Uchis</td>\n",
       "      <td>90</td>\n",
       "      <td>80</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>NEW</td>\n",
       "      <td>Classy 101</td>\n",
       "      <td>Feid x Young Miko</td>\n",
       "      <td>-</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>NEW</td>\n",
       "      <td>Bluffin</td>\n",
       "      <td>Gucci Mane &amp; Lil Baby</td>\n",
       "      <td>-</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Rank          Song                                            Artist  \\\n",
       "0           1    Last Night                                     Morgan Wallen   \n",
       "1           2      Fast Car                                        Luke Combs   \n",
       "2           3     Calm Down                               Rema & Selena Gomez   \n",
       "3           4       Flowers                                       Miley Cyrus   \n",
       "4           5   All My Life                        Lil Durk Featuring J. Cole   \n",
       "..        ...           ...                                               ...   \n",
       "95  RE- ENTRY  Angel, Pt. 1  Kodak Black, NLE Choppa, Jimin, JVKE & Muni Long   \n",
       "96        NEW  Girl In Mine                                          Parmalee   \n",
       "97         98     Moonlight                                        Kali Uchis   \n",
       "98        NEW    Classy 101                                 Feid x Young Miko   \n",
       "99        NEW       Bluffin                             Gucci Mane & Lil Baby   \n",
       "\n",
       "   LastWeekRank PeakRank WeeksOnboard  \n",
       "0             1        1           21  \n",
       "1             3        2           13  \n",
       "2             4        3           42  \n",
       "3             2        1           23  \n",
       "4             5        2            6  \n",
       "..          ...      ...          ...  \n",
       "95            -       65            2  \n",
       "96            -       97            1  \n",
       "97           90       80           11  \n",
       "98            -       99            1  \n",
       "99            -      100            1  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the song data\n",
    "\n",
    "bb_song100 = get_all_ele(1,'//div[@class=\"o-chart-results-list-row-container\"]',0)\n",
    "\n",
    "# Create Empty list for storing data\n",
    "bb_rank = []\n",
    "bb_song = []\n",
    "bb_sger = []\n",
    "bb_lweek  = []\n",
    "bb_peak = []\n",
    "bb_week = []\n",
    "\n",
    "# Extracting the data \n",
    "for song in bb_song100:\n",
    "    y = song.split('\\n')\n",
    "    \n",
    "    bb_rank.append(y[-6])\n",
    "    bb_song.append(y[-5])\n",
    "    bb_sger.append(y[-4])\n",
    "    bb_lweek.append(y[-3])\n",
    "    bb_peak.append(y[-2])\n",
    "    bb_week.append(y[-1])\n",
    "\n",
    "#Creating Dictionary & Storing in DataFrame'\n",
    "\n",
    "dict_bb = {'Rank': bb_rank, 'Song': bb_song, 'Artist': bb_sger, 'LastWeekRank': bb_lweek, 'PeakRank': bb_peak, 'WeeksOnboard':bb_week}\n",
    "\n",
    "bb_DF = pd.DataFrame(dict_bb)\n",
    "\n",
    "bb_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eacac24",
   "metadata": {},
   "source": [
    "6. Scrape the details of Highest sellingnovels.\n",
    "\n",
    "Url = https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\n",
    "\n",
    "You have to find the following details:\n",
    "\n",
    "A) Book name\n",
    "\n",
    "B) Author name\n",
    "\n",
    "C) Volumes sold\n",
    "\n",
    "D) Publisher\n",
    "\n",
    "E) Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "61068ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load URL, extract & store the elements\n",
    "load_url(url6)\n",
    "\n",
    "books = get_all_ele(1,'//td[@class=\"left\"]',0 )\n",
    "b_genre = get_all_ele(1,'//td[@class=\"last left\"]',0 )\n",
    "\n",
    "# Create empty list\n",
    "b_title = []\n",
    "b_author = []\n",
    "b_sales = []\n",
    "b_pub  = []\n",
    "\n",
    "# Extract an dappend in the list\n",
    "for bb in range(0,len(books)):\n",
    "    \n",
    "    if(bb%5 ==1):\n",
    "        b_title.append(books[bb])\n",
    "        \n",
    "    elif(bb%5 ==2):\n",
    "        b_author.append(books[bb])\n",
    "        \n",
    "    elif(bb%5 ==3):\n",
    "        b_sales.append(books[bb])\n",
    "        \n",
    "    elif(bb%5 ==4):\n",
    "        b_pub.append(books[bb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0236795c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Author</th>\n",
       "      <th>Vol. Sold</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Da Vinci Code,The</td>\n",
       "      <td>Brown, Dan</td>\n",
       "      <td>5,094,805</td>\n",
       "      <td>Transworld</td>\n",
       "      <td>Crime, Thriller &amp; Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Harry Potter and the Deathly Hallows</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,475,152</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Harry Potter and the Philosopher's Stone</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,200,654</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Harry Potter and the Order of the Phoenix</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,179,479</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fifty Shades of Grey</td>\n",
       "      <td>James, E. L.</td>\n",
       "      <td>3,758,936</td>\n",
       "      <td>Random House</td>\n",
       "      <td>Romance &amp; Sagas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Ghost,The</td>\n",
       "      <td>Harris, Robert</td>\n",
       "      <td>807,311</td>\n",
       "      <td>Random House</td>\n",
       "      <td>General &amp; Literary Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Happy Days with the Naked Chef</td>\n",
       "      <td>Oliver, Jamie</td>\n",
       "      <td>794,201</td>\n",
       "      <td>Penguin</td>\n",
       "      <td>Food &amp; Drink: General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Hunger Games,The:Hunger Games Trilogy</td>\n",
       "      <td>Collins, Suzanne</td>\n",
       "      <td>792,187</td>\n",
       "      <td>Scholastic Ltd.</td>\n",
       "      <td>Young Adult Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Lost Boy,The:A Foster Child's Search for the L...</td>\n",
       "      <td>Pelzer, Dave</td>\n",
       "      <td>791,507</td>\n",
       "      <td>Orion</td>\n",
       "      <td>Biography: General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Jamie's Ministry of Food:Anyone Can Learn to C...</td>\n",
       "      <td>Oliver, Jamie</td>\n",
       "      <td>791,095</td>\n",
       "      <td>Penguin</td>\n",
       "      <td>Food &amp; Drink: General</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title            Author  \\\n",
       "0                                   Da Vinci Code,The        Brown, Dan   \n",
       "1                Harry Potter and the Deathly Hallows     Rowling, J.K.   \n",
       "2            Harry Potter and the Philosopher's Stone     Rowling, J.K.   \n",
       "3           Harry Potter and the Order of the Phoenix     Rowling, J.K.   \n",
       "4                                Fifty Shades of Grey      James, E. L.   \n",
       "..                                                ...               ...   \n",
       "95                                          Ghost,The    Harris, Robert   \n",
       "96                     Happy Days with the Naked Chef     Oliver, Jamie   \n",
       "97              Hunger Games,The:Hunger Games Trilogy  Collins, Suzanne   \n",
       "98  Lost Boy,The:A Foster Child's Search for the L...      Pelzer, Dave   \n",
       "99  Jamie's Ministry of Food:Anyone Can Learn to C...     Oliver, Jamie   \n",
       "\n",
       "    Vol. Sold        Publisher                        Genre  \n",
       "0   5,094,805       Transworld  Crime, Thriller & Adventure  \n",
       "1   4,475,152       Bloomsbury           Children's Fiction  \n",
       "2   4,200,654       Bloomsbury           Children's Fiction  \n",
       "3   4,179,479       Bloomsbury           Children's Fiction  \n",
       "4   3,758,936     Random House              Romance & Sagas  \n",
       "..        ...              ...                          ...  \n",
       "95    807,311     Random House   General & Literary Fiction  \n",
       "96    794,201          Penguin        Food & Drink: General  \n",
       "97    792,187  Scholastic Ltd.          Young Adult Fiction  \n",
       "98    791,507            Orion           Biography: General  \n",
       "99    791,095          Penguin        Food & Drink: General  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store in dictionary & create DataFrame\n",
    "\n",
    "book_dict  ={'Title': b_title, 'Author': b_author, 'Vol. Sold': b_sales, 'Publisher': b_pub, 'Genre': b_genre}\n",
    "\n",
    "book_DF = pd.DataFrame(book_dict)\n",
    "\n",
    "book_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6315a1",
   "metadata": {},
   "source": [
    "7. Scrape the details most watched tv series of all time from imdb.com\n",
    "\n",
    "Url = https://www.imdb.com/list/ls095964455/\n",
    "\n",
    "You have to find the following details:\n",
    "\n",
    "A) Name\n",
    "\n",
    "B) Year span\n",
    "\n",
    "C) Genre\n",
    "\n",
    "D) Run time\n",
    "\n",
    "E) Ratings\n",
    "\n",
    "F) Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ec85d56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load URL\n",
    "load_url(url7)\n",
    "\n",
    "# Create Empty List to store the elements\n",
    "t_name = []\n",
    "t_span = []\n",
    "t_genr = []\n",
    "t_rtme = []\n",
    "t_rate = []\n",
    "t_vote = []\n",
    "\n",
    "# Extract the element data\n",
    "t_genr = get_all_ele(1, '//span[@class=\"genre\"]',0)\n",
    "t_rtme = get_all_ele(1, '//span[@class=\"runtime\"]',0)\n",
    "t_name_x = get_all_ele(1, '//h3[@class=\"lister-item-header\"]',0)\n",
    "\n",
    "# Extract relevant data\n",
    "for t in t_name_x:\n",
    "    \n",
    "    t_name.append(((t.split('(')[0]).split('.'))[1])\n",
    "    t_span.append((t.split('(')[1].replace(')','')))\n",
    "\n",
    "# Extract relevant data  \n",
    "t_rate_x = get_all_ele(1, '//span[@class=\"ipl-rating-star__rating\"]',0)\n",
    "\n",
    "# Extract relevant data\n",
    "for rt in range(0,len(t_rate_x)):\n",
    "    if(rt%23 == 0):\n",
    "        t_rate.append(t_rate_x[rt])\n",
    "\n",
    "# Extract relevant data\n",
    "t_vote_x = get_all_ele(1, '//p[@class=\"text-muted text-small\"]',0)\n",
    "\n",
    "i = 0\n",
    "for vt in t_vote_x:\n",
    "    z = vt.split(':')\n",
    "    i = i+1\n",
    "    \n",
    "    if(i%3 ==0):\n",
    "        t_vote.append(z[-1])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "45272893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Span</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Game of Thrones</td>\n",
       "      <td>2011–2019</td>\n",
       "      <td>Action, Adventure, Drama</td>\n",
       "      <td>57 min</td>\n",
       "      <td>9.2</td>\n",
       "      <td>2,174,806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stranger Things</td>\n",
       "      <td>2016–2024</td>\n",
       "      <td>Drama, Fantasy, Horror</td>\n",
       "      <td>51 min</td>\n",
       "      <td>8.7</td>\n",
       "      <td>1,252,501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Walking Dead</td>\n",
       "      <td>2010–2022</td>\n",
       "      <td>Drama, Horror, Thriller</td>\n",
       "      <td>44 min</td>\n",
       "      <td>8.1</td>\n",
       "      <td>1,032,998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13 Reasons Why</td>\n",
       "      <td>2017–2020</td>\n",
       "      <td>Drama, Mystery, Thriller</td>\n",
       "      <td>60 min</td>\n",
       "      <td>7.5</td>\n",
       "      <td>303,726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The 100</td>\n",
       "      <td>2014–2020</td>\n",
       "      <td>Drama, Mystery, Sci-Fi</td>\n",
       "      <td>43 min</td>\n",
       "      <td>7.6</td>\n",
       "      <td>262,863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Reign</td>\n",
       "      <td>2013–2017</td>\n",
       "      <td>Drama</td>\n",
       "      <td>42 min</td>\n",
       "      <td>7.4</td>\n",
       "      <td>51,989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>A Series of Unfortunate Events</td>\n",
       "      <td>2017–2019</td>\n",
       "      <td>Adventure, Comedy, Drama</td>\n",
       "      <td>50 min</td>\n",
       "      <td>7.8</td>\n",
       "      <td>64,015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Criminal Minds</td>\n",
       "      <td>2005–</td>\n",
       "      <td>Crime, Drama, Mystery</td>\n",
       "      <td>42 min</td>\n",
       "      <td>8.1</td>\n",
       "      <td>208,627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Scream: The TV Series</td>\n",
       "      <td>2015–2019</td>\n",
       "      <td>Comedy, Crime, Drama</td>\n",
       "      <td>45 min</td>\n",
       "      <td>7</td>\n",
       "      <td>43,416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>The Haunting of Hill House</td>\n",
       "      <td>2018</td>\n",
       "      <td>Drama, Horror, Mystery</td>\n",
       "      <td>572 min</td>\n",
       "      <td>8.6</td>\n",
       "      <td>260,436</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Name       Span                     Genre  \\\n",
       "0                   Game of Thrones   2011–2019  Action, Adventure, Drama   \n",
       "1                   Stranger Things   2016–2024    Drama, Fantasy, Horror   \n",
       "2                  The Walking Dead   2010–2022   Drama, Horror, Thriller   \n",
       "3                    13 Reasons Why   2017–2020  Drama, Mystery, Thriller   \n",
       "4                           The 100   2014–2020    Drama, Mystery, Sci-Fi   \n",
       "..                               ...        ...                       ...   \n",
       "95                            Reign   2013–2017                     Drama   \n",
       "96   A Series of Unfortunate Events   2017–2019  Adventure, Comedy, Drama   \n",
       "97                   Criminal Minds      2005–      Crime, Drama, Mystery   \n",
       "98            Scream: The TV Series   2015–2019      Comedy, Crime, Drama   \n",
       "99       The Haunting of Hill House        2018    Drama, Horror, Mystery   \n",
       "\n",
       "    Runtime Rating       Votes  \n",
       "0    57 min    9.2   2,174,806  \n",
       "1    51 min    8.7   1,252,501  \n",
       "2    44 min    8.1   1,032,998  \n",
       "3    60 min    7.5     303,726  \n",
       "4    43 min    7.6     262,863  \n",
       "..      ...    ...         ...  \n",
       "95   42 min    7.4      51,989  \n",
       "96   50 min    7.8      64,015  \n",
       "97   42 min    8.1     208,627  \n",
       "98   45 min      7      43,416  \n",
       "99  572 min    8.6     260,436  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store in Dictionary & Create DataFrame\n",
    "\n",
    "show_dict = {'Name': t_name, 'Span': t_span, 'Genre': t_genr, 'Runtime': t_rtme,'Rating': t_rate, 'Votes': t_vote }\n",
    "\n",
    "show_DF = pd.DataFrame(show_dict)\n",
    "\n",
    "show_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28917c97",
   "metadata": {},
   "source": [
    "8. Details of Datasetsfrom UCI machine learning repositories. \n",
    "\n",
    "Url = https://archive.ics.uci.edu/\n",
    "\n",
    "You have to find the following details:\n",
    "\n",
    "A) Dataset name\n",
    "\n",
    "B) Data type\n",
    "\n",
    "C) Task\n",
    "\n",
    "D) Attribute type\n",
    "\n",
    "E) No of instances\n",
    "\n",
    "F) No of attribute\n",
    "\n",
    "G) Year\n",
    "\n",
    "Note: - from the home page you have to go to the Show All Dataset page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebdb62db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception Raised at sub_val\n",
      "Tried again, not found\n",
      "Exception Raised at sub_val\n",
      "Tried again, not found\n",
      "Last page reached\n",
      "Exception Raised at sub_val\n",
      "Tried again, not found\n",
      "Exception Raised at sub_val\n",
      "Tried again, not found\n"
     ]
    }
   ],
   "source": [
    "# Load URL\n",
    "load_url(url8)\n",
    "\n",
    "# Click the reqd element to navigate to required page\n",
    "try:\n",
    "    sub_val(\"/html/body/div/div[1]/div[1]/div/div[2]/button\",1)\n",
    "    \n",
    "except:\n",
    "    print('Accept Cookies not occured')\n",
    "\n",
    "sub_val('/html/body/div/div[1]/div[1]/header/nav/ul/li[1]/a',1)\n",
    "time.sleep(2)\n",
    "\n",
    "# Create empty list for storing elements\n",
    "d_ext_inf = []\n",
    "d_dataset = []\n",
    "\n",
    "# Number of Datasets in page\n",
    "max_val = int(int(get_all_ele(1,'/html/body/div/div[1]/div[1]/main/div/div[2]/div[3]/p/span[3]',0)[0])/25)+1\n",
    "\n",
    "# Set number of elements in list & click next button\n",
    "\n",
    "sub_val('select-primary select select-sm rounded-full',0)\n",
    "time.sleep(1)\n",
    "sub_val('/html/body/div/div[1]/div[1]/main/div/div[2]/div[3]/label/select/option[5]',1)\n",
    "\n",
    "try:\n",
    "    sub_val('/html/body/div/div[1]/div[1]/main/div/div[2]/div[1]/div/label[2]/div[2]',1)\n",
    "    time.sleep(5)\n",
    "        \n",
    "except:\n",
    "    print('Element already clicked or not available')\n",
    "\n",
    "# store elements from entire page\n",
    "\n",
    "for i in range(0,max_val+1):\n",
    "    try:\n",
    "        d_dataset.append((get_all_ele(0,\"truncate\",0))[7:])\n",
    "        #d_ext_inf.append((get_all_ele(1, '//div[@class=\"grid grid-cols-8 overflow-x-auto\"]', 0)))\n",
    "        d_ext_inf.append((get_all_ele(1, '//tbody[@class=\"border\"]', 0)))\n",
    "        \n",
    "        \n",
    "    except:\n",
    "        print('Last page reached')\n",
    "    \n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        sub_val('/html/body/div/div[1]/div[1]/main/div/div[2]/div[3]/div/button[2]',1)\n",
    "        \n",
    "    except: \n",
    "        print('Next button Inactive, possibly end of datalist')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d730c4c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Type</th>\n",
       "      <th>Task</th>\n",
       "      <th>Tot Attr.</th>\n",
       "      <th>Inst</th>\n",
       "      <th>Attr</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Iris</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>Real</td>\n",
       "      <td>7/1/1988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Heart Disease</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>13</td>\n",
       "      <td>303</td>\n",
       "      <td>Categorical,Integer,Real</td>\n",
       "      <td>7/1/1988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adult</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>14</td>\n",
       "      <td>48.84K</td>\n",
       "      <td>Categorical,Integer</td>\n",
       "      <td>5/1/1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dry Bean Dataset</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>17</td>\n",
       "      <td>13.61K</td>\n",
       "      <td>Integer,Real</td>\n",
       "      <td>9/14/2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Diabetes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>20</td>\n",
       "      <td></td>\n",
       "      <td>Categorical,Integer</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>PMU-UD</td>\n",
       "      <td>Univariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>9</td>\n",
       "      <td>5.18K</td>\n",
       "      <td></td>\n",
       "      <td>8/5/2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>chestnut â€“ LARVIC</td>\n",
       "      <td>null</td>\n",
       "      <td>Classification, Clustering</td>\n",
       "      <td>3</td>\n",
       "      <td>1.45K</td>\n",
       "      <td></td>\n",
       "      <td>5/30/2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>EBL Domain Theories</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>Moral Reasoner</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>202</td>\n",
       "      <td>N/A</td>\n",
       "      <td>6/1/1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>DGP2 - The Second Data Generation Program</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Real</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>623 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Dataset          Type  \\\n",
       "0                                         Iris  Multivariate   \n",
       "1                                Heart Disease  Multivariate   \n",
       "2                                        Adult  Multivariate   \n",
       "3                             Dry Bean Dataset  Multivariate   \n",
       "4                                     Diabetes                 \n",
       "..                                         ...           ...   \n",
       "618                                     PMU-UD    Univariate   \n",
       "619                        chestnut â€“ LARVIC          null   \n",
       "620                        EBL Domain Theories                 \n",
       "621                             Moral Reasoner                 \n",
       "622  DGP2 - The Second Data Generation Program                 \n",
       "\n",
       "                           Task Tot Attr.    Inst                      Attr  \\\n",
       "0                Classification         4     150                      Real   \n",
       "1                Classification        13     303  Categorical,Integer,Real   \n",
       "2                Classification        14  48.84K       Categorical,Integer   \n",
       "3                Classification        17  13.61K              Integer,Real   \n",
       "4                                      20               Categorical,Integer   \n",
       "..                          ...       ...     ...                       ...   \n",
       "618              Classification         9   5.18K                             \n",
       "619  Classification, Clustering         3   1.45K                             \n",
       "620                                                                     N/A   \n",
       "621                                           202                       N/A   \n",
       "622                                                                    Real   \n",
       "\n",
       "          Year  \n",
       "0     7/1/1988  \n",
       "1     7/1/1988  \n",
       "2     5/1/1996  \n",
       "3    9/14/2020  \n",
       "4          N/A  \n",
       "..         ...  \n",
       "618   8/5/2018  \n",
       "619  5/30/2017  \n",
       "620        N/A  \n",
       "621   6/1/1994  \n",
       "622        N/A  \n",
       "\n",
       "[623 rows x 7 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Empty list to store required data filetered extracted \n",
    "d_name = []\n",
    "d_type = []\n",
    "d_task = []\n",
    "d_attn = []\n",
    "d_inst = []\n",
    "\n",
    "for d in d_dataset:\n",
    "    for ds in range(0,len(d)):\n",
    "        if(ds%10 ==0):\n",
    "            d_name.append((d[ds]))\n",
    "                          \n",
    "        elif(ds%10 == 2):\n",
    "            d_task.append(d[ds])\n",
    "        \n",
    "        elif(ds%10 == 3):\n",
    "            d_type.append(d[ds])\n",
    "        \n",
    "        elif(ds%10 == 4):\n",
    "            d_inst.append((d[ds].replace(' Instances', '')))\n",
    "        \n",
    "        elif(ds%10 == 5):\n",
    "            d_attn.append((d[ds].replace(' Attributes', '')))\n",
    "            \n",
    "d_year = []\n",
    "d_attr = []\n",
    "\n",
    "for d_ex in d_ext_inf:\n",
    "    for d in d_ex:\n",
    "        \n",
    "        if(len(d)>0):\n",
    "            x = ((d.split('\\n'))[-1])\n",
    "            d_year.append(x.split(' ')[-2])\n",
    "            \n",
    "        else:\n",
    "            d_year.append('-')\n",
    "          \n",
    "        y = d.split(' ')\n",
    "        z =''\n",
    "        for i in range(0,len(y)-3):\n",
    "            z = z+y[i+1]\n",
    "        d_attr.append(z.replace('Science',''))  \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "d_attr\n",
    "len(d_attr)\n",
    "\n",
    "# Store in Dictionary & create DataFrame\n",
    "\n",
    "dict_UCI = {'Dataset': d_name, 'Type': d_type, 'Task': d_task, 'Tot Attr.': d_attn, 'Inst': d_inst, 'Attr': d_attr, 'Year': d_year}\n",
    "\n",
    "dict_DF = pd.DataFrame(dict_UCI)\n",
    "\n",
    "dict_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1467f279",
   "metadata": {},
   "source": [
    "9. Scrape the details of Data science recruiters Url = https://www.naukri.com/hr-recruiters-consultants \n",
    "\n",
    "You have to find the following details: \n",
    "\n",
    "A) Name\n",
    "\n",
    "B) Designation\n",
    "\n",
    "C)Company \n",
    "\n",
    "D)Skills they hire for \n",
    "\n",
    "E) Location\n",
    "\n",
    "Note: - From naukri.com homepage click on the recruiters op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dc422218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception Raised at sub_val\n",
      "Tried again, not found\n"
     ]
    }
   ],
   "source": [
    "# Load URL & Navigate to required Pages\n",
    "\n",
    "load_url(url9)\n",
    "\n",
    "sub_val('/html/body/div[1]/div[4]/div[2]/nav/ul/li[2]/a',1)\n",
    "\n",
    "sub_val('/html/body/div/div[4]/div[2]/div[1]/div/span',1)\n",
    "\n",
    "send_val('Data Scientist','/html/body/div/div[4]/div[2]/div[1]/div/div/div[1]/div/div/div/input',1)\n",
    "\n",
    "sub_val('/html/body/div/div[4]/div[2]/div[1]/div/div/div[1]/div/div/div[2]/div[1]/ul/li[1]/div', 1)\n",
    "\n",
    "sub_val('/html/body/div/div[4]/div[2]/div[1]/div/button/span[2]', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "25ba3fcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Desgn</th>\n",
       "      <th>Comp</th>\n",
       "      <th>Location</th>\n",
       "      <th>Key Skill</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Analystics &amp; Modeling Specialist</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>This role will be a part of Survey Solutions a...</td>\n",
       "      <td>[Consulting, Machine learning, Open source, Py...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Fractal Analytics</td>\n",
       "      <td>Required Skills: . 5+ years of work experience...</td>\n",
       "      <td>[Supply chain, Analytical, Analytics, SQL, Tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>AMERICAN EXPRESS GLOBAL BUSINESS TRAVEL</td>\n",
       "      <td>Experience . 2+ years in an analytic role, pre...</td>\n",
       "      <td>[Data Science, Deep Learning, Data, Machine, D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Machine Learning (AI) Architect</td>\n",
       "      <td>Persistent</td>\n",
       "      <td>Qualifications Skills: BE/ BTech degree in Com...</td>\n",
       "      <td>[Intelligence, Time, Networking, Troubleshooti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lead Data Scientist</td>\n",
       "      <td>Morningstar</td>\n",
       "      <td>Working experience of Design patterns / System...</td>\n",
       "      <td>[Market Data, Linux, Process, Analytics, Data,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ACN - Applied Intelligence - Finance - Data Sc...</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>Quantexa Certification /Quantexa Experience (M...</td>\n",
       "      <td>[Applied Intelligence, Data processing, Analyt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Tata Nexarc</td>\n",
       "      <td>Tata Business Hub is looking for a competent D...</td>\n",
       "      <td>[Machine, Python, Science, Data mining, Machin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Oprable Inc</td>\n",
       "      <td>Analytics</td>\n",
       "      <td>[Data, Machine, SQL, Data analytics, Machine l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Director/Senior Director - Data Science</td>\n",
       "      <td>Axtria India</td>\n",
       "      <td>Familiarity with cloud technology such as AWS ...</td>\n",
       "      <td>[Data modeling, Data Pipeline, Data quality, I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Manager/Senior Manager - Data Science</td>\n",
       "      <td>Axtria India</td>\n",
       "      <td>Ability to build scalable models using Python,...</td>\n",
       "      <td>[Natural language processing, Data management,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Deputy Area Manager - B2C Underwriting Rural</td>\n",
       "      <td>Bajaj Finserv Ltd.</td>\n",
       "      <td>Qualifications : B-Tech / MBA Finance / Postgr...</td>\n",
       "      <td>[Underwriting, Logistics, B2C, Management, Log...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Deputy Area Manager - B2C Underwriting</td>\n",
       "      <td>Bajaj Finserv Ltd.</td>\n",
       "      <td>Build Stress Testing Framework and execute the...</td>\n",
       "      <td>[Underwriting, Logistics, B2C, Management, Log...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Data Scientist II, Tech</td>\n",
       "      <td>Uber</td>\n",
       "      <td>Basic Qualifications . MS, or Bachelors degree...</td>\n",
       "      <td>[Analytics, Data quality, Operations, Data, SQ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>ICON plc</td>\n",
       "      <td>Preferred Qualifications: . Ability to analyze...</td>\n",
       "      <td>[Clinical trials, Office, Data, MS Office, Cli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>ICON plc</td>\n",
       "      <td>Preferred Qualifications: . Ability to analyze...</td>\n",
       "      <td>[Clinical trials, Office, Public health, Healt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Franklin Templeton</td>\n",
       "      <td>Together, we will leverage data-informed insig...</td>\n",
       "      <td>[Asset Management, machine learning, Deep Lear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Lead- Data Scientist</td>\n",
       "      <td>Response Informatics</td>\n",
       "      <td>Experience working with pre-trained models, aw...</td>\n",
       "      <td>[Analytical, Processing, Deep Learning, Linear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Sr Data Scientist / Consultant</td>\n",
       "      <td>Curemd</td>\n",
       "      <td>Python</td>\n",
       "      <td>[Data mining, Program, SCALA, Machine learning...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>TRH Consultancy Services</td>\n",
       "      <td>Machine</td>\n",
       "      <td>[Python, R, Science, Tensorflow, Data Science]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>TRH Consultancy Services</td>\n",
       "      <td>Spacy</td>\n",
       "      <td>[Tensorflow, Pytorch, Ml, DBMS, SQL Server]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Desgn  \\\n",
       "0                    Analystics & Modeling Specialist   \n",
       "1                               Senior Data Scientist   \n",
       "2                               Senior Data Scientist   \n",
       "3                     Machine Learning (AI) Architect   \n",
       "4                                 Lead Data Scientist   \n",
       "5   ACN - Applied Intelligence - Finance - Data Sc...   \n",
       "6                                      Data Scientist   \n",
       "7                                      Data Scientist   \n",
       "8             Director/Senior Director - Data Science   \n",
       "9               Manager/Senior Manager - Data Science   \n",
       "10       Deputy Area Manager - B2C Underwriting Rural   \n",
       "11             Deputy Area Manager - B2C Underwriting   \n",
       "12                            Data Scientist II, Tech   \n",
       "13                                     Data Scientist   \n",
       "14                                     Data Scientist   \n",
       "15                                     Data Scientist   \n",
       "16                               Lead- Data Scientist   \n",
       "17                     Sr Data Scientist / Consultant   \n",
       "18                                     Data Scientist   \n",
       "19                                     Data Scientist   \n",
       "\n",
       "                                       Comp  \\\n",
       "0                                 Accenture   \n",
       "1                         Fractal Analytics   \n",
       "2   AMERICAN EXPRESS GLOBAL BUSINESS TRAVEL   \n",
       "3                                Persistent   \n",
       "4                               Morningstar   \n",
       "5                                 Accenture   \n",
       "6                               Tata Nexarc   \n",
       "7                               Oprable Inc   \n",
       "8                              Axtria India   \n",
       "9                              Axtria India   \n",
       "10                       Bajaj Finserv Ltd.   \n",
       "11                       Bajaj Finserv Ltd.   \n",
       "12                                     Uber   \n",
       "13                                 ICON plc   \n",
       "14                                 ICON plc   \n",
       "15                       Franklin Templeton   \n",
       "16                     Response Informatics   \n",
       "17                                   Curemd   \n",
       "18                 TRH Consultancy Services   \n",
       "19                 TRH Consultancy Services   \n",
       "\n",
       "                                             Location  \\\n",
       "0   This role will be a part of Survey Solutions a...   \n",
       "1   Required Skills: . 5+ years of work experience...   \n",
       "2   Experience . 2+ years in an analytic role, pre...   \n",
       "3   Qualifications Skills: BE/ BTech degree in Com...   \n",
       "4   Working experience of Design patterns / System...   \n",
       "5   Quantexa Certification /Quantexa Experience (M...   \n",
       "6   Tata Business Hub is looking for a competent D...   \n",
       "7                                           Analytics   \n",
       "8   Familiarity with cloud technology such as AWS ...   \n",
       "9   Ability to build scalable models using Python,...   \n",
       "10  Qualifications : B-Tech / MBA Finance / Postgr...   \n",
       "11  Build Stress Testing Framework and execute the...   \n",
       "12  Basic Qualifications . MS, or Bachelors degree...   \n",
       "13  Preferred Qualifications: . Ability to analyze...   \n",
       "14  Preferred Qualifications: . Ability to analyze...   \n",
       "15  Together, we will leverage data-informed insig...   \n",
       "16  Experience working with pre-trained models, aw...   \n",
       "17                                             Python   \n",
       "18                                            Machine   \n",
       "19                                              Spacy   \n",
       "\n",
       "                                            Key Skill  \n",
       "0   [Consulting, Machine learning, Open source, Py...  \n",
       "1   [Supply chain, Analytical, Analytics, SQL, Tro...  \n",
       "2   [Data Science, Deep Learning, Data, Machine, D...  \n",
       "3   [Intelligence, Time, Networking, Troubleshooti...  \n",
       "4   [Market Data, Linux, Process, Analytics, Data,...  \n",
       "5   [Applied Intelligence, Data processing, Analyt...  \n",
       "6   [Machine, Python, Science, Data mining, Machin...  \n",
       "7   [Data, Machine, SQL, Data analytics, Machine l...  \n",
       "8   [Data modeling, Data Pipeline, Data quality, I...  \n",
       "9   [Natural language processing, Data management,...  \n",
       "10  [Underwriting, Logistics, B2C, Management, Log...  \n",
       "11  [Underwriting, Logistics, B2C, Management, Log...  \n",
       "12  [Analytics, Data quality, Operations, Data, SQ...  \n",
       "13  [Clinical trials, Office, Data, MS Office, Cli...  \n",
       "14  [Clinical trials, Office, Public health, Healt...  \n",
       "15  [Asset Management, machine learning, Deep Lear...  \n",
       "16  [Analytical, Processing, Deep Learning, Linear...  \n",
       "17  [Data mining, Program, SCALA, Machine learning...  \n",
       "18     [Python, R, Science, Tensorflow, Data Science]  \n",
       "19        [Tensorflow, Pytorch, Ml, DBMS, SQL Server]  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the element Data and store in list\n",
    "\n",
    "comp_data = get_all_ele(0,'jobTupleHeader',0)\n",
    "skill_data = get_all_ele(0, 'jobTuple',0)\n",
    "\n",
    "# Create an Empty List\n",
    "desg = []\n",
    "comp_name = []\n",
    "location = []\n",
    "skills = []\n",
    "\n",
    "# Extract and store data in list\n",
    "for data in skill_data:\n",
    "    desg.append(data.split('\\n')[0])\n",
    "    comp_name.append(data.split('\\n')[1])\n",
    "    location.append(data.split('\\n')[7])\n",
    "    skills.append(data.split('\\n')[9:-2])\n",
    "    \n",
    "# Store in Dictionary & Create DataFrame\n",
    "\n",
    "dict_nkr = {'Desgn': desg, 'Comp': comp_name, 'Location': location, 'Key Skill': skills }\n",
    "\n",
    "nfr_DF = pd.DataFrame(dict_nkr)\n",
    "\n",
    "nfr_DF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
